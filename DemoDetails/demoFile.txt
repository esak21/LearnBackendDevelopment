Create an EC2 Instance 

SSh 22 TCP 80 , 3030 5601 8081 8085 8086 9200 
Create Key Pair 

Login to the Machine 

sudo yum update -y 
sudo yum install -y docker 
sudo service docker start 
sudo usermod -a -G docker ec2-user

sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose


Install Java17 in AWS Linux 
sudo dnf install java-17-amazon-corretto-devel -y 
INstall maven 
sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo
sudo sed -i s/\$releasever/6/g /etc/yum.repos.d/epel-apache-maven.repo
sudo yum install -y apache-maven
mvn â€“version
--------------------------------------------------------------------------------
sudo yum install git -y 
git clone -b kafka_conenct https://github.com/esak21/LearnBackendDevelopment.git
-------------------------------------------------------



Logout and Login Back to same Ec2 
docker info 

check docker is installed 

docker run -d -p 80:5000 training/webapp:latest python app.py 
docker run -d -p 80:80 --name nginx nginx 





docker-compose version

/usr/lib/jvm/jdk-17/
sudo nano /etc/environment export JAVA_HOME=/usr/lib/jvm/jdk-17/


mvn compile exec:java -Dexec.mainClass=org.example.coffeeOrderCoffeeOrderProducerApplication 


mvn compile exec:java -Dexec.mainClass=com.bookapp.example.booksproducer.BooksproducerApplication

Control Center Login 
http://localhost:9021/

confluent-hub install confluentinc/kafka-connect-aws-lambda:2.0.6
confluent-hub install confluentinc/kafka-connect-s3:10.5.1
confluent-hub install confluentinc/kafka-connect-elasticsearch:14.0.6

---------------------------------------------------------

Kafka to ElasticSearch Sink 


curl -s -i -X PUT -H  "Content-Type:application/json" \
    http://localhost:8083/connectors/sink-elastic-01/config \
    -d '{
            "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
            "connection.url": "http://172.18.0.3:9200",
            "type.name": "_doc",
            "topics": "books-events-1",
            "key.ignore": "true",
            "schema.ignore": "true",
            "value.converter": "org.apache.kafka.connect.json.JsonConverter",
            "value.converter.schemas.enable": "false"
            }'



  "name": "elasticsearch-sink",
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max": "1",
    "topics": "example-topic",
    "key.ignore": "true",
    "schema.ignore": "true",
    "connection.url": "http://localhost:9200",
    "type.name": "_doc",
    "name": "elasticsearch-sink",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false"


curl -X POST http://54.226.212.153:8083/connectors -H "Content-Type: application/json" -d '{
"name":"elasticsearch-sink21",
    "config" : {
    "connector.class":"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max":"1",
    "topics" : "coffee-orders-demo",
    "connection.url": "http://54.226.212.153:9200",
    "type.name":"_doc",
    "key.ignore" : "true",
     "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "key.converter.schemas.enable": "false"
    }
}'

-------------------------------------------------------------------------------------------
Working 

curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
"name":"elasticsearch-sink2",
    "config" : {
    "connector.class":"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max":"1",
    "topics":"bathirdparty-library-events",
    "connection.url": "http://3.82.174.234:9200",
    "type.name":"_doc",
    "key.ignore":"true",
    "schema.ignore":"true",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false"
    }
}'

-------------------------------------------------------------------------------------------
============
S3

Working S3 Module 

name=s3-sinlconnector
connector.class=io.confluent.connect.s3.S3SinkConnector
s3.region=us-east-1
flush.size=3
schema.compatibility=NONE
tasks.max=1
topics=books-events
s3.part.size=5242880
s3.prefix=/connect-test
format.class=io.confluent.connect.s3.format.json.JsonFormat
partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner
value.converter=org.apache.kafka.connect.storage.StringConverter
storage.class=io.confluent.connect.s3.storage.S3Storage
s3.bucket.name=kafka-conenct-example
aws.access.key.id=AKIAYPWFPPZXPTBWZ35K
aws.secret.access.key=cIKU38L8uxfX2Va3xWxPPDsKYMb6kd7jRBW5ulEq



value.converter=org.apache.kafka.connect.avro.avroconverter
storage.class=io.confluent.connect.s3.storage.S3Storage

key.converter = org.apache.kafka.connect.json.JsonConverter,                                                                                                                                                                                                 
key.converter.schemas.enable=false
value.converter.schemas.enable=false

===========================================================================








connector.class=io.confluent.connect.aws.dynamodb.DynamoDbSinkConnector
tasks.max=1
topics=books-events-1
aws.dynamodb.region=us-east-1
aws.dynamodb.endpoint=https://dynamodb.us-east-1.amazonaws.com
aws.dynamodb.pk.hash=value.bookid
aws.dynamodb.pk.sort=
table.name.format=kafka_${topic}
transforms=flatten
transforms.flatten.type=org.apache.kafka.connect.transforms.Flatten$Value
transforms.flatten.delimiter=_
key.converter.schemas.enable=false
value.converter.schemas.enable=false
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
aws.access.key.id=AKIAYPWFPPZXPTBWZ35K
aws.secret.access.key=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




name=s3-sink
connector.class=io.confluent.connect.s3.S3SinkConnector
tasks.max=1
topics=bathirdparty-library-events
s3.region=us-east-1
s3.bucket.name=kafka-connect-example
s3.part.size=5242880
flush.size=3
storage.class=io.confluent.connect.s3.storage.S3Storage
partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner
schema.compatibility=NONE


{
  "connector.class": "io.confluent.connect.s3.S3SinkConnector",
  "format.class": "io.confluent.connect.s3.format.json.JsonFormat",
  "flush.size": "100000",
  "topics": "TopicName_S3SinkConnector",
  "tasks.max": "1",
  "storage.class": "io.confluent.connect.s3.storage.S3Storage",
  "AWS_ACCESS_KEY": "AKIAYPWFPPZXIRUWCUXK",
  "AWS_SECRET_KEY": "zbEepZONB4ATr0i59tSeSvM5umCaIQnbKkgo2kpR",
  "s3.bucket.name": "kafka-conenct-example"
}


++++++++++++++++++++++

{                                                                                                                                                                                                                                                                   
  "name": "meetups-to-s3",                                                                                                                                                                                                                                          
  "config": {                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                    
    "name": "coffeeOrders-to-s3",                                                                                                                                                                                              
    "connector.class":"io.confluent.connect.s3.S3SinkConnector",                                                                                                                                                                                                   
    "tasks.max":"1",                                                                                                                                                                                                                                                

    "topics":"coffee-orders-demo",                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                    
    "s3.bucket.name":"kafka-conenct-example",                                                                                                                                                                                                                         

    "s3.region":"us-east-1",                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                    

    "s3.part.size":"5242880",                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                    

    "flush.size":"100000",                                                                                                                                                                                                                                          
                                                                                                                                                                                                                                                                    

    "key.converter":"org.apache.kafka.connect.json.JsonConverter",                                                                                                                                                                                                  
    "key.converter.schemas.enable":"false",                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                    

    "value.converter":"org.apache.kafka.connect.json.JsonConverter",                                                                                                                                                                                                
    "value.converter.schemas.enable":"false",                                                                                                                                                                                                                       

    "storage.class":"io.confluent.connect.s3.storage.S3Storage",                                                                                                                                                                                                    

    "format.class":"io.confluent.connect.s3.format.json.JsonFormat",                                                                                                                                                                                                

    "schema.compatibility":"NONE",                                                                                                                                                                                                                                  

    "partitioner.class":"io.confluent.connect.storage.partitioner.TimeBasedPartitioner",                                                                                                                                                                            

    "locale":"en",

    "timezone":"UTC",                                                                                                                                                                                                                                               

    "path.format":"'date'=YYYY-MM-dd/'hour'=HH",                                                                                                                                                                                                                    

    "partition.duration.ms":"3600000",                                                                                                                                                                                                                              

    "rotate.interval.ms":"60000",                                                                                                                                                                                                                                   

    "timestamp.extractor":"Record"                                                                                                                                                                                                                                  
  }                                                                                                                                                                                                                                                                 
} 



{"error":{"root_cause":[{"type":"mapper_parsing_exception","reason":"Root mapping definition has unsupported parameters:  [type : text] [fields : {keyword={ignore_above=256, type=keyword}}]"}],"type":"mapper_parsing_exception","reason":"Root mapping definition has unsupported parameters:  [type : text] [fields : {keyword={ignore_above=256, type=keyword}}]"},"status":400}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:346)


